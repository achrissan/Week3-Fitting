{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Fitting\n",
    "\n",
    "In many different cases, we might have a model for how a system works, and want to fit that model to a set of observations. \n",
    "\n",
    "We're going to investigate the process of fitting using a classic paper that proposed a model for the [T cell receptor](https://www.ncbi.nlm.nih.gov/pubmed/11606269). Here, the authors develop a mathematical model for how binding occurs and then have observations of how much binding occurs under specific conditions. Identifying whether and how this model fits has led to a better understanding of how our immune system recognizes diseased cells, and how to design T cells that respond to diseases like cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "np.seterr(over='raise')\n",
    "\n",
    "\n",
    "def Req_func(Phisum, Rtot, L0, KxStar, f, Ka):\n",
    "    \"\"\" Mass balance. Transformation to account for bounds. \"\"\"\n",
    "    Req = Rtot / (1.0 + L0 * f * Ka * (1 + Phisum) ** (f - 1))\n",
    "    return Phisum - Ka * KxStar * Req\n",
    "\n",
    "\n",
    "def StoneMod(Rtot: float, Kd: float, v, Kx: float, L0: np.ndarray):\n",
    "    '''\n",
    "    Returns the number of mutlivalent ligand bound to a cell with Rtot\n",
    "    receptors, granted each epitope of the ligand binds to the receptor\n",
    "    kind in question with dissociation constant Kd and cross-links with\n",
    "    other receptors with crosslinking constant Kx. All eq derived from Stone et al. (2001).\n",
    "    '''\n",
    "    v = np.int_(v)\n",
    "    assert L0.shape == v.shape\n",
    "\n",
    "    if Rtot <= 0.0:\n",
    "        raise RuntimeError(\"You input a negative amount of receptor.\")\n",
    "    \n",
    "    if Kx <= 0.0:\n",
    "        raise RuntimeError(\"You input a negative Kx.\")\n",
    "\n",
    "    if np.amin(L0) <= 0.0:\n",
    "        raise RuntimeError(\"You input a negative L0.\")\n",
    "\n",
    "    Ka = 1.0 / Kd\n",
    "    KxStar = Kx / Ka\n",
    "\n",
    "    ## Solve Req by calling least_squares\n",
    "    lsq = least_squares(Req_func, np.ones_like(L0), jac=\"cs\",\n",
    "                        max_nfev=5000, xtol=1.0E-10, ftol=1.0E-10, gtol=1.0E-10,\n",
    "                        args=(Rtot, L0, KxStar, v, Ka))\n",
    "\n",
    "    if lsq['success'] is False:\n",
    "        print(lsq)\n",
    "        raise RuntimeError(\"Failure in solving for Req. If you see this message contact Dr. Meyer.\")\n",
    "\n",
    "    Phisum = lsq.x\n",
    "\n",
    "    # Calculate L, according to equation 7\n",
    "    Lbound = L0 / KxStar * ((1 + Phisum) ** v - 1)\n",
    "\n",
    "    # Calculate Rmulti from equation 5\n",
    "    Rmulti = L0 / KxStar * v * Phisum * ((1 + Phisum) ** (v - 1) - 1)\n",
    "\n",
    "    # Calculate Rbound\n",
    "    Rbnd = L0 / KxStar * v * Phisum * (1 + Phisum) ** (v - 1)\n",
    "\n",
    "    return Lbound, Rbnd, Rmulti\n",
    "\n",
    "Xs = np.array([8.1E-11, 3.4E-10, 1.3E-09, 5.7E-09, 2.1E-08, 8.7E-08, 3.4E-07, 1.5E-06, 5.7E-06, 2.82E-11, 1.17E-10, 4.68E-10, 1.79E-09, 7.16E-09, 2.87E-08, 1.21E-07, 4.5E-07, 1.87E-06, 1.64E-11, 6.93E-11, 2.58E-10, 1.11E-09, 4.35E-09, 1.79E-08, 7.38E-08, 2.9E-07, 1.14E-06])\n",
    "Ys = np.array([-196, -436, 761, 685, 3279, 7802, 11669, 12538, 9012, -1104, -769, 1455, 2693, 7134, 11288, 14498, 16188, 13237, 988, 1734, 4491, 9015, 13580, 17159, 18438, 18485, 17958])\n",
    "Vs = np.repeat([2, 3, 4], 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) We will fit the data contained within Fig. 3B. Plot this data and describe the relationship you see between Kx, Kd, and valency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Note that Xs are the concentrations from the experiment\n",
    "# Vs are the complex valencies\n",
    "# Ys are the measurements of response\n",
    "# Each of these vectors is the same length, with each position describing an experimental measurement\n",
    "\n",
    "def plot_valency_data(Xs_in, Ys_in, Vs_in, format='o-'):\n",
    "    \"\"\"If you pass in your real or simulated data, this will plot it for you.\"\"\"\n",
    "    colors = ['r', 'g', 'b']\n",
    "\n",
    "    for valency in range(3):\n",
    "        plt.semilogx(Xs_in[Vs_in == valency + 2], \n",
    "                     Ys_in[Vs_in == valency + 2],\n",
    "                     colors[valency] + format)\n",
    "            \n",
    "    plt.xlim([1E-11, 1E-5])\n",
    "    plt.xticks([1E-10, 1E-8, 1E-6])\n",
    "    plt.xlabel('[Oligomer] M')\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) First, to do so, we'll need a function that takes the model predictions, scales them to the units of the actual measurements, and finds the predictions for each condition. Define a scaling parameter and a function that takes it along with the other parameters to make predictions about the experiment.\n",
    "\n",
    "Use the fit parameters shown in Table 1 (row 2) and overlay with the measurements to ensure your function is working. (Scale = 1 for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Here is a scaling function\n",
    "# Note that the unknown parameters are passed in as an array\n",
    "# This will make working with least_squares() easier later\n",
    "# parameters = [scale, Kd (M), Kx (cell)]\n",
    "def scale_StoneMod(parameters, Vs, Xs):\n",
    "    Rtot = 24000.0 # cell^-1\n",
    "    Lbound, Rbnd, Rmulti = StoneMod(Rtot, parameters[1], Vs, parameters[2], Xs)\n",
    "    return parameters[0] * Rmulti\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Now use `scipy.optimize.least_squares` to find the least squares solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# You will need a function that returns the residuals:\n",
    "def residuals(parameters, Vs_in, Xs_in, Ys_in):\n",
    "    return Ys_in - scale_StoneMod(parameters, Vs_in, Xs_in)\n",
    "\n",
    "# Note that the data (Xs, Ys) are not unknowns, so they should be passed in using the args argument of least_squares\n",
    "\n",
    "# With this function, your least squares call should look something like this:\n",
    "# lsq_solution = least_squares(residuals, params_guess, args=(Vs, Xs, Ys))\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Using leave-one-out crossvalidation, does this model predict the data? Plot the measured vs. predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn provides a great interface for getting indices that help you split the data\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "for train_idx, test_idx in LeaveOneOut().split(Vs):\n",
    "    Vs_training = Vs[train_idx]\n",
    "    Vs_testing = Vs[test_idx]\n",
    "\n",
    "    # Xs_training...\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Using bootstrap estimation, plot the confidence interval of the model predictions along with the data points.\n",
    "\"Confidence interval\" does not have a precise definition. For example, you could show the interval over which 50% of the bootstrap samples fall (25th to 75th quantile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a very useful resample function\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# For example, to resample the data, you can do this:\n",
    "resamp = resample(range(Vs.size))\n",
    "Vs_resampled = Vs[resamp]\n",
    "Xs_resampled = Xs[resamp]\n",
    "Ys_resampled = Ys[resamp]\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) _Generally_, how would you expect the cross-validation and bootstrap results to change if you had fewer data points?\n",
    "\n",
    "Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Now, we will perform a local sensitivity analysis to look at the dependence of the model results on each parameter. Vary each parameter up and down relative to its optimum by 10-fold **while holding the others constant**, and plot the sum of squared error. Which parameter affects the error the most? Which one the least?\n",
    "\n",
    "You should get a plot that roughly looks like a \"U\" for each parameter. Use a log-log plot to show the variation the best (`plt.loglog`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.6.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
